{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script performs the analysis of training a convolutional neural network and predicting y given X, where y and X are synthetic benchmark datasets, \n",
    "# as descirbed in Mamalakis et al. 2022. We also apply DeepSHAP to explain the predictions of the network. \n",
    "\n",
    " \n",
    "# citation: \n",
    "# Mamalakis, A., E.A. Barnes, I. Ebert-Uphoff (2022) “Investigating the fidelity of explainable \n",
    "# artificial intelligence methods for application of convolutional neural networks in geoscience,” \n",
    "# arXiv preprint https://arxiv.org/abs/2202.03407. \n",
    "\n",
    "# Editor: Dr Antonios Mamalakis (amamalak@colostate.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 81334,
     "status": "ok",
     "timestamp": 1587745911654,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhxGyQiqej0v5Oem2a-mzfI3Kg0wR7mkUvEhP32K4w=s64",
      "userId": "08898050549780290733"
     },
     "user_tz": 360
    },
    "id": "V4SwBF1lVec0",
    "outputId": "b0609276-0f0c-414a-d98f-fd813d4ccd95"
   },
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# IMPORT STATEMENTS\n",
    "#.............................................\n",
    "\n",
    "# local env is AIgeo_new\n",
    "\n",
    "#General Python math functions\n",
    "import math\n",
    "#Loading in data (netcdf files)\n",
    "import h5py\n",
    "#Handling data\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "#Plotting figures\n",
    "import matplotlib.pyplot as plt #Main plotting package\n",
    "\n",
    "#machine learning package\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior() \n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "#Interpreting neural networks \n",
    "import  shap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# LOAD DATA\n",
    "#.............................................\n",
    "\n",
    "# load matlab data with the synthetic benchmark\n",
    "# This data was generated using the matlab script Gen_Synth_SHAPES\n",
    "\n",
    "filepath = 'synth_data_shapes.mat'\n",
    "DATA = {}\n",
    "f = h5py.File(filepath)\n",
    "for k, v in f.items():\n",
    "    DATA[k] = np.array(v)\n",
    " \n",
    "InputX = np.array(DATA['X'])\n",
    "lats = np.array(DATA['lat'])\n",
    "lons= np.array(DATA['lon'])\n",
    "y_synth = np.array(DATA['y'])\n",
    "Cnt_tr = np.array(DATA['Cnt'])\n",
    "print('data is loaded') # print message 'data is loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# DATA MANIPULATION AND SANITY PLOT\n",
    "#.............................................\n",
    "\n",
    "Cnt_tr=np.swapaxes(Cnt_tr,-1,1)\n",
    "InputX=np.swapaxes(InputX,-1,1)\n",
    "\n",
    "lats=lats.flatten()\n",
    "lons=lons.flatten()\n",
    "#Flatten the y time series \n",
    "y_synth=y_synth.flatten()\n",
    "\n",
    "#sanity plot (just for checking I have read the data correclty)\n",
    "X, Y = np.meshgrid(lons, lats) \n",
    "cs = plt.contourf(X, Y, Cnt_tr[9], cmap =\"jet\")   \n",
    "cbar = plt.colorbar(cs)   \n",
    "plt.title('matplotlib.pyplot.contourf() Example') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CY3xayjPtCil"
   },
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# PREPARE THE DATA FOR TRAINING\n",
    "#.............................................\n",
    "\n",
    "# Rename the sst array to X (inputs) and Y (labels) to stick with machine learning convention\n",
    "X_all = np.copy(InputX[...,np.newaxis])\n",
    "Y_all = np.copy(y_synth)\n",
    "\n",
    "# Change the Y (label) array values to 1 if the sample is above 0 and 0 if the sample is below\n",
    "Y_all[Y_all > 0] = 1 # square frames cover more area \n",
    "Y_all[Y_all <= 0] = 0 # circular frames cover more area\n",
    "\n",
    "# Convert the Y array into a categorical array. \n",
    "Y_all = tf.keras.utils.to_categorical(Y_all)\n",
    "\n",
    "# Set the fraction of samples that will be used for validation\n",
    "frac_validate = 0.1\n",
    "\n",
    "# Separate the X and Y matrices into training and validation sub-sets\n",
    "# For this problem, we will take the last fraction_validate fraction of samples as our validation dataset\n",
    "X_train = X_all[:int(-frac_validate*len(X_all))]\n",
    "Y_train = Y_all[:int(-frac_validate*len(Y_all))]\n",
    "\n",
    "X_validation = X_all[int(-frac_validate*len(X_all)):]\n",
    "Y_validation = Y_all[int(-frac_validate*len(Y_all)):]\n",
    "\n",
    "#Create class weights for training the model. If the dataset is unbalanced, this helps ensure the model\n",
    "# does not simply start guessing the class that has more samples.\n",
    "#class_weight = class_weight_creator(Y_train)\n",
    "\n",
    "#Calculate the number of inputs into the neural network (this will be helpful later on)\n",
    "# This value is the number of latitudes times the number of longitudes\n",
    "number_inputs = X_all.shape[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# BUILD THE CONVOLUTIONAL NEURAL NETWORK\n",
    "#.............................................\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32,(5,5),strides=(2,2),activation='relu',padding='same',input_shape=number_inputs))\n",
    "#model.add(tf.keras.layers.Conv2D(32,(5,5),strides=(1,1),activation='relu',padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Conv2D(32,(5,5),strides=(1,1),activation='relu',padding='same'))\n",
    "#model.add(tf.keras.layers.Conv2D(32,(5,5),strides=(1,1),activation='relu',padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Conv2D(64,(3,3),strides=(1,1),activation='relu',padding='same'))\n",
    "model.add(tf.keras.layers.MaxPooling2D(2))\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "model.add(tf.keras.layers.Dense(units=128,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=64,activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(units=2,activation='softmax'))\n",
    "#model.add(tf.keras.layers.Dense(1,activation='linear',use_bias=False))\n",
    "\n",
    "#Define the learning rate of the neural network\n",
    "learning_rate = 0.01\n",
    "\n",
    "# We will use the stochastic gradient descent (SGD) optimizer, because we have control over\n",
    "# the learning rate and it is effective for our problem.\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(lr=learning_rate),\n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics=['accuracy'] )\n",
    "\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# UNCOMMENT TO TRAIN THE NEURAL NETWORK\n",
    "#.............................................\n",
    "\n",
    "#batch_size = 128 #The number of samples the network sees before it backpropagates (batch size)\n",
    "#epochs =  10 #The number of times the network will loop through the entire dataset (epochs)\n",
    "#shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "#verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n",
    "\n",
    "###Train the neural network!\n",
    "#model.fit(X_train, Y_train, validation_data=(X_validation, Y_validation), \n",
    "#          batch_size=batch_size, epochs=epochs, shuffle=shuffle, verbose=verbose) #, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# LOAD ALREADY TRAINED MODEL\n",
    "#.............................................\n",
    "\n",
    "\n",
    "# load model, including its weights and the optimizer\n",
    "model = tf.keras.models.load_model('my_model_shapes.h5')\n",
    "# Show the model architecture\n",
    "model.summary()\n",
    "# loss and accuracy in \"new model\"\n",
    "loss, acc = model.evaluate(X_validation, Y_validation, verbose=2)\n",
    "print('Restored model, categorical crossentropy: ', loss)\n",
    "print('Restored model, categorical accuracy: ', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# GET EXPLANATIONS FROM SHAP\n",
    "#.............................................\n",
    "\n",
    "import shap\n",
    "\n",
    "# select a set of background examples to take an expectation over\n",
    "background = X_train[np.random.choice(X_train.shape[0], 5000, replace=False)]\n",
    "#background=np.zeros((1,X_train.shape[1]*X_train.shape[2],1))\n",
    "\n",
    "# explain predictions of the model on three images\n",
    "e = shap.DeepExplainer(model, background)\n",
    "# ...or pass tensors directly\n",
    "# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get explanations\n",
    "shap_values = e.shap_values(X_validation[[344,3566],:,:])\n",
    "\n",
    "# plot the feature attributions\n",
    "shap.image_plot(shap_values, -X_validation[[344,3566],:,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values1 = np.array(shap_values)\n",
    "\n",
    "shap_values1 = np.copy(shap_values1[:,:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# SAVE SHAP RESULTS\n",
    "#............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'SHAP.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 2) # this is essentially number of samples \n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "classes = ds.createDimension('classes',2)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('SHAP', 'f4', ('time', 'classes','lat', 'lon'))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(shap_values1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ann_ENSO_example.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "dabdd342f67897c34373f41b22b6abd9c2f79b58187b71b8676727362b44975f"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('AIgeo_new': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
