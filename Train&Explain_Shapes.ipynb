{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This script performs the analysis of training a convolutional neural network and predicting y given X, where y and X are synthetic benchmark datasets of output and input respectevely, \n",
    "# as descirbed in Mamalakis et al. 2022. Here, we also apply many XAI mehtods to explain the predictions of the network. \n",
    "\n",
    " \n",
    "# citation: \n",
    "# Mamalakis, A., E.A. Barnes, I. Ebert-Uphoff (2022) “Investigating the fidelity of explainable \n",
    "# artificial intelligence methods for application of convolutional neural networks in geoscience,” \n",
    "# arXiv preprint https://arxiv.org/abs/2202.03407. \n",
    "\n",
    "# Editor: Dr Antonios Mamalakis (amamalak@colostate.edu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 81334,
     "status": "ok",
     "timestamp": 1587745911654,
     "user": {
      "displayName": "Elizabeth Barnes",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhxGyQiqej0v5Oem2a-mzfI3Kg0wR7mkUvEhP32K4w=s64",
      "userId": "08898050549780290733"
     },
     "user_tz": 360
    },
    "id": "V4SwBF1lVec0",
    "outputId": "b0609276-0f0c-414a-d98f-fd813d4ccd95"
   },
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# IMPORT STATEMENTS\n",
    "#.............................................\n",
    "\n",
    "# local env is AIgeo2\n",
    "\n",
    "#General Python math functions\n",
    "import math\n",
    "#Loading in data (netcdf files)\n",
    "import h5py\n",
    "#Handling data\n",
    "import numpy as np\n",
    "import netCDF4 as nc\n",
    "#Plotting figures\n",
    "import matplotlib.pyplot as plt #Main plotting package\n",
    "\n",
    "# neural networks package\n",
    "import keras\n",
    "\n",
    "#Explaining neural networks using LRP\n",
    "import innvestigate \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# LOAD DATA\n",
    "#.............................................\n",
    "\n",
    "# load matlab data with the synthetic benchmark\n",
    "# This data was generated using the matlab script Gen_Synth_SHAPES\n",
    "filepath = 'synth_data_shapes.mat'\n",
    "DATA = {}\n",
    "f = h5py.File(filepath)\n",
    "for k, v in f.items():\n",
    "    DATA[k] = np.array(v)\n",
    "\n",
    "InputX = np.array(DATA['X'])\n",
    "lats = np.array(DATA['lat'])\n",
    "lons= np.array(DATA['lon'])\n",
    "y_synth = np.array(DATA['y'])\n",
    "Cnt_tr = np.array(DATA['Cnt'])\n",
    "print('data is loaded') # print message 'data is loaded'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# DATA MANIPULATION AND SANITY PLOT\n",
    "#.............................................\n",
    "\n",
    "Cnt_tr=np.swapaxes(Cnt_tr,-1,1)\n",
    "InputX=np.swapaxes(InputX,-1,1)\n",
    "\n",
    "lats=lats.flatten()\n",
    "lons=lons.flatten()\n",
    "#Flatten the y time series \n",
    "y_synth=y_synth.flatten()\n",
    "\n",
    "#sanity plot (just for checking I have read the data correclty)\n",
    "X, Y = np.meshgrid(lons, lats) \n",
    "cs = plt.contourf(X, Y, Cnt_tr[9], cmap =\"jet\")   \n",
    "cbar = plt.colorbar(cs)   \n",
    "plt.title('matplotlib.pyplot.contourf() Example') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CY3xayjPtCil"
   },
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# PREPARE THE DATA FOR TRAINING\n",
    "#.............................................\n",
    "\n",
    "\n",
    "# Rename the array to X (inputs) and Y (labels) to stick with machine learning convention\n",
    "X_all = np.copy(InputX[...,np.newaxis])\n",
    "Y_all = np.copy(y_synth)\n",
    "\n",
    "# Change the Y (label) array values to 1 if the sample is above 0 and 0 if the sample is below\n",
    "Y_all[Y_all > 0] = 1 # square frames cover more area \n",
    "Y_all[Y_all <= 0] = 0 # circular frames cover more area\n",
    "\n",
    "# Convert the Y array into a categorical array. \n",
    "Y_all = keras.utils.to_categorical(Y_all)\n",
    "\n",
    "# Set the fraction of samples that will be used for validation\n",
    "frac_validate = 0.1\n",
    "\n",
    "# Separate the X and Y matrices into training and validation sub-sets\n",
    "# For this problem, we will take the last fraction_validate fraction of samples as our validation dataset\n",
    "X_train = X_all[:int(-frac_validate*len(X_all))]\n",
    "Y_train = Y_all[:int(-frac_validate*len(Y_all))]\n",
    "\n",
    "X_validation = X_all[int(-frac_validate*len(X_all)):]\n",
    "Y_validation = Y_all[int(-frac_validate*len(Y_all)):]\n",
    "\n",
    "# Create class weights for training the model. If the dataset is unbalanced, this helps ensure the model\n",
    "# does not simply start guessing the class that has more samples.\n",
    "# class_weight = class_weight_creator(Y_train)\n",
    "\n",
    "# Calculate the number of inputs into the neural network (this will be helpful for the next code cell)\n",
    "# This value is the number of latitudes times the number of longitudes\n",
    "number_inputs = X_all.shape[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# BUILD THE CONVOLUTIONAL NEURAL NETWORK\n",
    "#.............................................\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "model.add(keras.layers.Conv2D(32,(5,5),strides=(2,2),activation='relu',padding='same',input_shape=number_inputs))\n",
    "#model.add(keras.layers.Conv2D(32,(5,5),strides=(1,1),activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Conv2D(32,(5,5),strides=(1,1),activation='relu',padding='same'))\n",
    "#model.add(keras.layers.Conv2D(32,(5,5),strides=(1,1),activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Conv2D(64,(3,3),strides=(1,1),activation='relu',padding='same'))\n",
    "model.add(keras.layers.MaxPooling2D(2))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.Dense(units=128,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=64,activation='relu'))\n",
    "model.add(keras.layers.Dense(units=2,activation='softmax'))\n",
    "\n",
    "#Define the learning rate of the neural network\n",
    "learning_rate = 0.01\n",
    "\n",
    "# We will use the stochastic gradient descent (SGD) optimizer, because we have control over\n",
    "# the learning rate and it is effective for our problem.\n",
    "\n",
    "model.compile(optimizer=keras.optimizers.SGD(lr=learning_rate),\n",
    "              loss = 'categorical_crossentropy', \n",
    "              metrics=['accuracy'] )\n",
    "\n",
    "model.summary()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# TRAIN THE NEURAL NETWORK\n",
    "#.............................................\n",
    "\n",
    "batch_size = 128 #The number of samples the network sees before it backpropagates (batch size)\n",
    "epochs =  5 #The number of times the network will loop through the entire dataset (epochs)\n",
    "shuffle = True #Set whether to shuffle the training data so the model doesn't see it sequentially \n",
    "verbose = 2 #Set whether the model will output information when trained (0 = no output; 2 = output accuracy every epoch)\n",
    "\n",
    "###Train the neural network!\n",
    "model.fit(X_train, Y_train, validation_data=(X_validation, Y_validation), \n",
    "          batch_size=batch_size, epochs=epochs, shuffle=shuffle, verbose=verbose) #, class_weight=class_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# SAVE THE TRAINED NETWORK\n",
    "#.............................................\n",
    "\n",
    "model.save('my_model_shapes.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model, including its weights and the optimizer\n",
    "model = keras.models.load_model('my_model_shapes.h5')\n",
    "# Show the model architecture\n",
    "model.summary()\n",
    "# loss and accuracy in \"new model\"\n",
    "loss, acc = model.evaluate(X_validation, Y_validation, verbose=2)\n",
    "print('Restored model, categorical crossentropy: ', loss)\n",
    "print('Restored model, categorical accuracy: ', acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-GBMcJUm67D-"
   },
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# GET EXPLANATIONS FROM MANY XAI METHODS\n",
    "#.............................................\n",
    "\n",
    "\n",
    "# Use innvestigate package\n",
    "# Remove the softmax layer from the model\n",
    "model_nosoftmax = innvestigate.utils.model_wo_softmax(model)\n",
    "\n",
    "#Create the \"analyzer\", or the object that will generate the heatmaps given an input sample\n",
    "PN_analyzer = innvestigate.create_analyzer('pattern.net', model_nosoftmax)\n",
    "PN_analyzer.fit(X_train)\n",
    "\n",
    "PA_analyzer = innvestigate.create_analyzer('pattern.attribution', model_nosoftmax)\n",
    "PA_analyzer.fit(X_train)\n",
    "\n",
    "DT_analyzer = innvestigate.create_analyzer('deep_taylor', model_nosoftmax)\n",
    "\n",
    "lrp_analyzer = innvestigate.create_analyzer('lrp.alpha_beta', model_nosoftmax, alpha=1,beta=0)\n",
    "\n",
    "lrp2_analyzer = innvestigate.create_analyzer('lrp.epsilon', model_nosoftmax,epsilon=10**(-7))\n",
    "\n",
    "lrp3_analyzer = innvestigate.create_analyzer('lrp.z', model_nosoftmax)\n",
    "\n",
    "lrpSeqA_analyzer = innvestigate.create_analyzer('lrp.sequential_preset_a', model_nosoftmax)\n",
    "\n",
    "lrpSeqAflat_analyzer = innvestigate.create_analyzer('lrp.sequential_preset_a_flat', model_nosoftmax)\n",
    "\n",
    "gradtinput_analyzer = innvestigate.create_analyzer('input_t_gradient', model_nosoftmax)\n",
    "\n",
    "intgrad_analyzer = innvestigate.create_analyzer('integrated_gradients', model_nosoftmax)\n",
    "\n",
    "grad_analyzer = innvestigate.create_analyzer('gradient', model_nosoftmax)\n",
    "\n",
    "smoothgrad_analyzer = innvestigate.create_analyzer('smoothgrad', model_nosoftmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# XAI LOOP\n",
    "#.............................................\n",
    "\n",
    "#First, create some lists to hold the output\n",
    "PN_heatmaps1_all = []\n",
    "PA_heatmaps1_all = []\n",
    "DT_heatmaps1_all = []\n",
    "LRP_heatmaps1_all = []\n",
    "LRP2_heatmaps1_all = []\n",
    "LRP3_heatmaps1_all = []\n",
    "LRPSEQA_heatmaps1_all = []\n",
    "LRPSEQAFLAT_heatmaps1_all = []\n",
    "ItG_heatmaps1_all = []\n",
    "intG_heatmaps1_all = []\n",
    "Grad_heatmaps1_all = []\n",
    "SmooG_heatmaps1_all = []\n",
    "\n",
    "\n",
    "prediction_all=[]\n",
    "\n",
    "\n",
    "#We will process all of the testing samples\n",
    "for sample_ind, sample in enumerate(X_validation):\n",
    "\n",
    "      # prediction from the model\n",
    "      sample_prediction = np.argmax(model.predict(sample[np.newaxis,...]))\n",
    "\n",
    "      PN_heatmap1 = PN_analyzer.analyze(sample[np.newaxis,...])\n",
    "      PA_heatmap1 = PA_analyzer.analyze(sample[np.newaxis,...])\n",
    "      DT_heatmap1 = DT_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      LRP_heatmap1 = lrp_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      LRP2_heatmap1 = lrp2_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      LRP3_heatmap1 = lrp3_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      LRPSEQA_heatmap1 = lrpSeqA_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      LRPSEQAFLAT_heatmap1 = lrpSeqAflat_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      ItG_heatmap1 = gradtinput_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      intG_heatmap1 = intgrad_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      Grad_heatmap1 = grad_analyzer.analyze(sample[np.newaxis,...]) \n",
    "      SmooG_heatmap1 = smoothgrad_analyzer.analyze(sample[np.newaxis,...])\n",
    "\n",
    "\n",
    "      PN_heatmaps1_all.append(PN_heatmap1)\n",
    "      PA_heatmaps1_all.append(PA_heatmap1)\n",
    "      DT_heatmaps1_all.append(DT_heatmap1)\n",
    "      LRP_heatmaps1_all.append(LRP_heatmap1)\n",
    "      LRP2_heatmaps1_all.append(LRP2_heatmap1)\n",
    "      LRP3_heatmaps1_all.append(LRP3_heatmap1)\n",
    "      LRPSEQA_heatmaps1_all.append(LRPSEQA_heatmap1)\n",
    "      LRPSEQAFLAT_heatmaps1_all.append(LRPSEQAFLAT_heatmap1)\n",
    "      ItG_heatmaps1_all.append(ItG_heatmap1)\n",
    "      intG_heatmaps1_all.append(intG_heatmap1)\n",
    "      Grad_heatmaps1_all.append(Grad_heatmap1)\n",
    "      SmooG_heatmaps1_all.append(SmooG_heatmap1)\n",
    "\n",
    "      prediction_all.append(sample_prediction)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# NEXT WE PERFORM DATA MANIPULATION TO GET ALL XAI RESULTS BACK TO THE 'GLOBE' FORMAT\n",
    "#............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert the list to an array\n",
    "\n",
    "PN_heatmaps1_all = np.array(PN_heatmaps1_all)\n",
    "PA_heatmaps1_all = np.array(PA_heatmaps1_all)\n",
    "DT_heatmaps1_all = np.array(DT_heatmaps1_all)\n",
    "LRP_heatmaps1_all = np.array(LRP_heatmaps1_all)\n",
    "LRP2_heatmaps1_all = np.array(LRP2_heatmaps1_all)\n",
    "LRP3_heatmaps1_all = np.array(LRP3_heatmaps1_all)\n",
    "LRPSEQA_heatmaps1_all = np.array(LRPSEQA_heatmaps1_all)\n",
    "LRPSEQAFLAT_heatmaps1_all = np.array(LRPSEQAFLAT_heatmaps1_all)\n",
    "ItG_heatmaps1_all = np.array(ItG_heatmaps1_all)\n",
    "intG_heatmaps1_all = np.array(intG_heatmaps1_all)\n",
    "Grad_heatmaps1_all = np.array(Grad_heatmaps1_all)\n",
    "SmooG_heatmaps1_all = np.array(SmooG_heatmaps1_all)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PN_heatmaps1_all =np.swapaxes(PN_heatmaps1_all ,1,2)\n",
    "PN_heatmaps1_all =np.swapaxes(PN_heatmaps1_all ,2,3)\n",
    "\n",
    "PA_heatmaps1_all =np.swapaxes(PA_heatmaps1_all ,1,2)\n",
    "PA_heatmaps1_all =np.swapaxes(PA_heatmaps1_all ,2,3)\n",
    "\n",
    "DT_heatmaps1_all =np.swapaxes(DT_heatmaps1_all ,1,2)\n",
    "DT_heatmaps1_all =np.swapaxes(DT_heatmaps1_all ,2,3)\n",
    "\n",
    "LRP_heatmaps1_all =np.swapaxes(LRP_heatmaps1_all ,1,2)\n",
    "LRP_heatmaps1_all =np.swapaxes(LRP_heatmaps1_all ,2,3)\n",
    "\n",
    "LRP2_heatmaps1_all =np.swapaxes(LRP2_heatmaps1_all ,1,2)\n",
    "LRP2_heatmaps1_all =np.swapaxes(LRP2_heatmaps1_all ,2,3)\n",
    "\n",
    "LRP3_heatmaps1_all =np.swapaxes(LRP3_heatmaps1_all ,1,2)\n",
    "LRP3_heatmaps1_all =np.swapaxes(LRP3_heatmaps1_all ,2,3)\n",
    "\n",
    "LRPSEQA_heatmaps1_all =np.swapaxes(LRPSEQA_heatmaps1_all ,1,2)\n",
    "LRPSEQA_heatmaps1_all =np.swapaxes(LRPSEQA_heatmaps1_all ,2,3)\n",
    "\n",
    "LRPSEQAFLAT_heatmaps1_all =np.swapaxes(LRPSEQAFLAT_heatmaps1_all ,1,2)\n",
    "LRPSEQAFLAT_heatmaps1_all =np.swapaxes(LRPSEQAFLAT_heatmaps1_all ,2,3)\n",
    "\n",
    "ItG_heatmaps1_all =np.swapaxes(ItG_heatmaps1_all ,1,2)\n",
    "ItG_heatmaps1_all =np.swapaxes(ItG_heatmaps1_all ,2,3)\n",
    "\n",
    "intG_heatmaps1_all =np.swapaxes(intG_heatmaps1_all ,1,2)\n",
    "intG_heatmaps1_all =np.swapaxes(intG_heatmaps1_all ,2,3)\n",
    "\n",
    "Grad_heatmaps1_all =np.swapaxes(Grad_heatmaps1_all ,1,2)\n",
    "Grad_heatmaps1_all =np.swapaxes(Grad_heatmaps1_all ,2,3)\n",
    "\n",
    "SmooG_heatmaps1_all =np.swapaxes(SmooG_heatmaps1_all ,1,2)\n",
    "SmooG_heatmaps1_all =np.swapaxes(SmooG_heatmaps1_all ,2,3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PN_heatmaps1=np.copy(PN_heatmaps1_all[:,:,:,0,0])\n",
    "PA_heatmaps1=np.copy(PA_heatmaps1_all[:,:,:,0,0])\n",
    "DT_heatmaps1=np.copy(DT_heatmaps1_all[:,:,:,0,0])\n",
    "LRP_heatmaps1=np.copy(LRP_heatmaps1_all[:,:,:,0,0])\n",
    "LRP2_heatmaps1=np.copy(LRP2_heatmaps1_all[:,:,:,0,0])\n",
    "LRP3_heatmaps1=np.copy(LRP3_heatmaps1_all[:,:,:,0,0])\n",
    "LRPSEQA_heatmaps1=np.copy(LRPSEQA_heatmaps1_all[:,:,:,0,0])\n",
    "LRPSEQAFLAT_heatmaps1=np.copy(LRPSEQAFLAT_heatmaps1_all[:,:,:,0,0])\n",
    "ItG_heatmaps1=np.copy(ItG_heatmaps1_all[:,:,:,0,0])\n",
    "intG_heatmaps1=np.copy(intG_heatmaps1_all[:,:,:,0,0])\n",
    "Grad_heatmaps1=np.copy(Grad_heatmaps1_all[:,:,:,0,0])\n",
    "SmooG_heatmaps1=np.copy(SmooG_heatmaps1_all[:,:,:,0,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# NEXT WE SAVE ALL XAI RESULTS\n",
    "#............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'PN.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('PN', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(PN_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'PA.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('PA', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(PA_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'DT.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('DT', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(DT_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRP.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRP', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRP_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPe.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPe', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRP2_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPz.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPz', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRP3_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPseqA.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPseqA', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRPSEQA_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPseqAflat.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPseqAflat', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRPSEQAFLAT_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'ItG.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('ItG', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(ItG_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'intG.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('intG', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(intG_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'Grad.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('Grad', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(Grad_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'SmooG.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('SmooG', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(SmooG_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'predictions.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 50000)\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "\n",
    "value2 = ds.createVariable('y_hat_NN', 'f4', ('time',))\n",
    "value2.units = 'unitless'\n",
    "\n",
    "value2[:] = np.array(prediction_all)\n",
    "\n",
    "print('var size ', value2.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# NEXT WE APPLY XAI METHODS TO EXPLAIN THE 345th TESTING SAMPLE ONLY\n",
    "#............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove the softmax layer from the model\n",
    "model_nosoftmax = innvestigate.utils.model_wo_softmax(model)\n",
    "\n",
    "#Create the \"analyzer\" and allow the user to select which class they want to explain\n",
    "PN_analyzer0 = innvestigate.create_analyzer('pattern.net', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "PN_analyzer0.fit(X_train)\n",
    "\n",
    "PA_analyzer0 = innvestigate.create_analyzer('pattern.attribution', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "PA_analyzer0.fit(X_train)\n",
    "\n",
    "DT_analyzer0 = innvestigate.create_analyzer('deep_taylor', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "\n",
    "lrp_analyzer0 = innvestigate.create_analyzer('lrp.alpha_beta', model_nosoftmax, alpha=1,beta=0, neuron_selection_mode=\"index\")\n",
    "\n",
    "lrp2_analyzer0 = innvestigate.create_analyzer('lrp.epsilon', model_nosoftmax,epsilon=10**(-7), neuron_selection_mode=\"index\")\n",
    "\n",
    "lrp3_analyzer0 = innvestigate.create_analyzer('lrp.z', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "\n",
    "lrpSeqA_analyzer0 = innvestigate.create_analyzer('lrp.sequential_preset_a', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "\n",
    "lrpSeqAflat_analyzer0 = innvestigate.create_analyzer('lrp.sequential_preset_a_flat', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "\n",
    "gradtinput_analyzer0 = innvestigate.create_analyzer('input_t_gradient', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "\n",
    "intgrad_analyzer0 = innvestigate.create_analyzer('integrated_gradients', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "\n",
    "grad_analyzer0 = innvestigate.create_analyzer('gradient', model_nosoftmax, neuron_selection_mode=\"index\")\n",
    "\n",
    "smoothgrad_analyzer0 = innvestigate.create_analyzer('smoothgrad', model_nosoftmax, neuron_selection_mode=\"index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain the 345th testing sample\n",
    "# and specifically why the class that corresponds to \"squares\" received very small likelihood\n",
    "# the \"squares\" class is 1.\n",
    "      \n",
    "PN0_heatmap1 = PN_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "PA0_heatmap1 = PA_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "DT0_heatmap1 = DT_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "LRP0_heatmap1 = lrp_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "LRP20_heatmap1 = lrp2_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "LRP30_heatmap1 = lrp3_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "LRPSEQA0_heatmap1 = lrpSeqA_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "LRPSEQAFLAT0_heatmap1 = lrpSeqAflat_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "ItG0_heatmap1 = gradtinput_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "intG0_heatmap1 = intgrad_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "Grad0_heatmap1 = grad_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n",
    "SmooG0_heatmap1 = smoothgrad_analyzer0.analyze(X_validation[344][np.newaxis,...],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#.............................................\n",
    "# NEXT WE SAVE ALL XAI RESULTS\n",
    "#............................................."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PN0_heatmaps1 = np.copy(PN0_heatmap1[:,:,:,0])\n",
    "PA0_heatmaps1 = np.copy(PA0_heatmap1[:,:,:,0])\n",
    "DT0_heatmaps1 = np.copy(DT0_heatmap1[:,:,:,0])\n",
    "LRP0_heatmaps1 = np.copy(LRP0_heatmap1[:,:,:,0])\n",
    "LRP20_heatmaps1 = np.copy(LRP20_heatmap1[:,:,:,0])\n",
    "LRP30_heatmaps1 = np.copy(LRP30_heatmap1[:,:,:,0])\n",
    "LRPSEQA0_heatmaps1 = np.copy(LRPSEQA0_heatmap1[:,:,:,0])\n",
    "LRPSEQAFLAT0_heatmaps1 = np.copy(LRPSEQAFLAT0_heatmap1[:,:,:,0])\n",
    "ItG0_heatmaps1 = np.copy(ItG0_heatmap1[:,:,:,0])\n",
    "intG0_heatmaps1 = np.copy(intG0_heatmap1[:,:,:,0])\n",
    "Grad0_heatmaps1 = np.copy(Grad0_heatmap1[:,:,:,0])\n",
    "SmooG0_heatmaps1 = np.copy(SmooG0_heatmap1[:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'PN0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('PN0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(PN0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'PA0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('PA0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(PA0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'DT0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('DT0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(DT0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRP0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRP0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRP0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPe0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPe0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRP20_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPz0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPz0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRP30_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPseqA0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPseqA0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRPSEQA0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'LRPseqAflat0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('LRPseqAflat0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(LRPSEQAFLAT0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'ItG0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('ItG0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(ItG0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'intG0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('intG0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(intG0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'Grad0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('Grad0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(Grad0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = 'SmooG0.nc'\n",
    "ds = nc.Dataset(fn, 'w', format='NETCDF4')\n",
    "\n",
    "time = ds.createDimension('time', 1)\n",
    "lat = ds.createDimension('lat', 65)\n",
    "lon = ds.createDimension('lon', 65)\n",
    "\n",
    "times = ds.createVariable('time', 'f4', ('time',))\n",
    "latss = ds.createVariable('lat', 'f4', ('lat',))\n",
    "lonss = ds.createVariable('lon', 'f4', ('lon',))\n",
    "value = ds.createVariable('SmooG0', 'f4', ('time', 'lat', 'lon',))\n",
    "value.units = 'unitless'\n",
    "\n",
    "latss[:] = np.copy(lats)\n",
    "lonss[:] = np.copy(lons)\n",
    "value[:] = np.copy(SmooG0_heatmaps1)\n",
    "\n",
    "print('var size ', value.shape)\n",
    "ds.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ann_ENSO_example.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "b4bdbb5f427c26be90f7fbe7db0e747a4e5bef84d2226ff1bfcfe268d4a7d6db"
  },
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('AIgeo': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
